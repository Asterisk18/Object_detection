{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a595e19",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 324\u001b[39m\n\u001b[32m    321\u001b[39m     cv2.destroyAllWindows()\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 241\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    237\u001b[39m os.makedirs(\u001b[33m'\u001b[39m\u001b[33msamples\u001b[39m\u001b[33m'\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# Capture a single frame from the video stream\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     ret, frame = \u001b[43mvideo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m    243\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Exit loop if frame could not be read\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "class RobustObjectTracker:\n",
    "    \"\"\"Improved object tracker with better separation of nearby objects\"\"\"\n",
    "    \n",
    "    def __init__(self, history_length=10, iou_threshold=0.3, min_persistence=5, \n",
    "                 max_disappeared=10, min_distance=50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            history_length: Frames to keep in memory\n",
    "            iou_threshold: Minimum overlap to consider same object\n",
    "            min_persistence: No. of frames to be observed before confirming an object\n",
    "            max_disappeared: No. of frames after which an object is removed from consideration\n",
    "            min_distance: Minimum distance to consider objects separate (in pixels)\n",
    "        \"\"\"\n",
    "        self.tracked_objects = {}\n",
    "        self.next_id = 0\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.min_persistence = min_persistence\n",
    "        self.max_disappeared = max_disappeared\n",
    "        self.min_distance = min_distance\n",
    "        \n",
    "    def calculate_iou(self, box1, box2):\n",
    "        \"\"\"Calculate Intersection over Union between two boxes\"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        \n",
    "        inter = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1) # this calculates the value of intersection area\n",
    "        area1 = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "        area2 = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "        union = area1 + area2 - inter\n",
    "        \n",
    "        return inter / union if union > 0 else 0\n",
    "    \n",
    "    # def calculate_distance(self, box1, box2):\n",
    "    #     \"\"\"Calculate center distance between two boxes\"\"\"\n",
    "    #     c1 = ((box1[0]+box1[2])/2, (box1[1]+box1[3])/2)\n",
    "    #     c2 = ((box2[0]+box2[2])/2, (box2[1]+box2[3])/2)\n",
    "    #     return np.sqrt((c1[0]-c2[0])**2 + (c1[1]-c2[1])**2)\n",
    "    \n",
    "    def non_max_suppression(self, boxes):\n",
    "        \"\"\"Prevent merging of nearby objects based on center distance and box area\"\"\"\n",
    "\n",
    "        # If there is only one or no box, no suppression is needed\n",
    "        if len(boxes) <= 1:\n",
    "            return boxes\n",
    "\n",
    "        # Convert list of boxes to a NumPy array for easier vectorized computation\n",
    "        boxes = np.array(boxes)\n",
    "\n",
    "        # Compute the center point (cx, cy) of each box\n",
    "        centers = np.column_stack((\n",
    "            (boxes[:, 0] + boxes[:, 2]) / 2,  # center x\n",
    "            (boxes[:, 1] + boxes[:, 3]) / 2   # center y\n",
    "        ))\n",
    "\n",
    "        # Initialize a distance matrix to store pairwise center distances\n",
    "        dist_matrix = np.zeros((len(boxes), len(boxes)))\n",
    "        \n",
    "        # Calculate pairwise Euclidean distances between centers\n",
    "        for i in range(len(boxes)):\n",
    "            for j in range(i + 1, len(boxes)):\n",
    "                dist = np.linalg.norm(centers[i] - centers[j])\n",
    "                dist_matrix[i, j] = dist\n",
    "                dist_matrix[j, i] = dist  # symmetric\n",
    "\n",
    "        # Initialize a set to store indexes of boxes to be removed\n",
    "        to_remove = set()\n",
    "\n",
    "        # Compare all unique pairs of boxes\n",
    "        for i in range(len(boxes)):\n",
    "            for j in range(i + 1, len(boxes)):\n",
    "                # If the distance between centers is below threshold, consider suppression\n",
    "                if dist_matrix[i, j] < self.min_distance:\n",
    "                    # Calculate area of both boxes\n",
    "                    area_i = (boxes[i, 2] - boxes[i, 0]) * (boxes[i, 3] - boxes[i, 1])\n",
    "                    area_j = (boxes[j, 2] - boxes[j, 0]) * (boxes[j, 3] - boxes[j, 1])\n",
    "\n",
    "                    # Remove the smaller box to avoid duplicate tracking\n",
    "                    if area_i > area_j:\n",
    "                        to_remove.add(j)\n",
    "                    else:\n",
    "                        to_remove.add(i)\n",
    "\n",
    "        # Return only boxes that are not marked for removal\n",
    "        return [box for i, box in enumerate(boxes) if i not in to_remove]\n",
    "\n",
    "    \n",
    "    def update_tracker(self, current_boxes):\n",
    "        \"\"\"Main tracking update method: matches current detections to existing tracks,\n",
    "        updates existing tracks, initializes new tracks, and removes stale ones.\"\"\"\n",
    "\n",
    "        # Step 1: Apply Non-Max Suppression to remove overlapping boxes that are too close\n",
    "        current_boxes = self.non_max_suppression(current_boxes)\n",
    "\n",
    "        # Step 2: Initialize data structures for matched tracks and detections\n",
    "        matched_detections = set()  # indexes of current_boxes that were matched\n",
    "        matched_tracks = set()      # object IDs of tracks that were matched\n",
    "        matches = []                # list of (object_id, detection_box) tuples\n",
    "\n",
    "        # Step 3: Perform IoU-based matching if we have both detections and existing tracks\n",
    "        if current_boxes and self.tracked_objects:\n",
    "            # Create cost matrix: rows = existing tracks, columns = current detections\n",
    "            cost_matrix = np.zeros((len(self.tracked_objects), len(current_boxes)))\n",
    "\n",
    "            # Fill cost matrix with IoU values between tracked boxes and detected boxes\n",
    "            for i, (obj_id, obj_data) in enumerate(self.tracked_objects.items()):\n",
    "                for j, det_box in enumerate(current_boxes):\n",
    "                    cost_matrix[i, j] = self.calculate_iou(obj_data['last_box'], det_box)\n",
    "\n",
    "            # Step 3a: Use Hungarian algorithm to assign tracks to detections optimally\n",
    "            # (maximize IoU, hence we minimize negative IoU)\n",
    "            row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n",
    "\n",
    "            # Step 3b: Filter valid matches based on IoU threshold\n",
    "            for i, j in zip(row_ind, col_ind):\n",
    "                if cost_matrix[i, j] > self.iou_threshold: # if cost is greater than the threshold i.e. this object was tracked before, so it has same object id as the before one\n",
    "                    obj_id = list(self.tracked_objects.keys())[i] # copying the same object id\n",
    "                    matches.append((obj_id, current_boxes[j])) # adding it to the matched_object list\n",
    "                    matched_tracks.add(obj_id) # adding its id to matched object list\n",
    "                    matched_detections.add(j)\n",
    "\n",
    "        # Step 4: updating the info of the object in the main data based on the latest observed frame\n",
    "        for obj_id, box in matches:\n",
    "            self.tracked_objects[obj_id]['last_box'] = box # storing the last observed box\n",
    "            self.tracked_objects[obj_id]['disappeared'] = 0 # resetting the disappering counter to zero\n",
    "            self.tracked_objects[obj_id]['history'].append(box) # adding current object rectangle to the corresponding object history \n",
    "\n",
    "        # Step 5: For unmatched detections, create new tracks with a new ID\n",
    "        for j, box in enumerate(current_boxes):\n",
    "            if j not in matched_detections:\n",
    "                # initiating new object instance for the unmatched objects\n",
    "                self.tracked_objects[self.next_id] = { \n",
    "                    'last_box': box,\n",
    "                    'history': [box],\n",
    "                    'disappeared': 0,\n",
    "                    'label': None,\n",
    "                    'confidence': 0\n",
    "                }\n",
    "                self.next_id += 1\n",
    "\n",
    "        # Step 6: For unmatched existing tracks, increase disappearance count\n",
    "        # and remove them if they've disappeared for too long\n",
    "        for obj_id in list(self.tracked_objects.keys()):\n",
    "            if obj_id not in matched_tracks:\n",
    "                self.tracked_objects[obj_id]['disappeared'] += 1\n",
    "                if self.tracked_objects[obj_id]['disappeared'] > self.max_disappeared:\n",
    "                    del self.tracked_objects[obj_id]\n",
    "\n",
    "        # Step 7: Filter and return only those tracks that have persisted long enough\n",
    "        confirmed_objects = {}\n",
    "        for obj_id, obj_data in self.tracked_objects.items():\n",
    "            if len(obj_data['history']) >= self.min_persistence:\n",
    "                confirmed_objects[obj_id] = obj_data\n",
    "\n",
    "        return confirmed_objects\n",
    "\n",
    "def auto_canny(image, sigma=0.2):\n",
    "    \"\"\"Automatic Canny edge detection\"\"\"\n",
    "    v = np.median(image)\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    return cv2.Canny(image, lower, upper)\n",
    "\n",
    "def detect_objects(frame):\n",
    "    \"\"\"Improved object detection with better contour filtering\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8))\n",
    "    equalized = clahe.apply(gray)\n",
    "    \n",
    "    img_h, img_w = gray.shape\n",
    "    kernel_size = max(3, int(min(img_h, img_w) * 0.002)) | 1\n",
    "    blurred = cv2.GaussianBlur(equalized, (kernel_size, kernel_size), 0)\n",
    "    \n",
    "    edges = auto_canny(blurred)\n",
    "    \n",
    "    morph_kernel = max(3, int(min(img_h, img_w) * 0.005)) | 1\n",
    "    kernel = np.ones((morph_kernel, morph_kernel), np.uint8)\n",
    "    closed = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    contours, _ = cv2.findContours(closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    min_area = max(500, img_h * img_w * 0.0005)\n",
    "    max_area = img_h * img_w * 0.9\n",
    "    boxes = []\n",
    "    \n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        aspect_ratio = w / float(h)\n",
    "        \n",
    "        if (min_area < area < max_area and \n",
    "            0.2 < aspect_ratio < 5 and # following checks are added to keep the object which are seperate from the edges\n",
    "            x > 5 and y > 5 and \n",
    "            x + w < img_w - 5 and \n",
    "            y + h < img_h - 5):\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    \"\"\"Extract HOG features for classification\"\"\"\n",
    "    gray = rgb2gray(image)\n",
    "    features = hog(\n",
    "        gray,\n",
    "        orientations=9,\n",
    "        pixels_per_cell=(8, 8),\n",
    "        cells_per_block=(2, 2),\n",
    "        block_norm='L2-Hys',\n",
    "        visualize=False\n",
    "    )\n",
    "    return features\n",
    "\n",
    "def main():\n",
    "    # Load pre-trained object classification model from disk\n",
    "    model = joblib.load(\"classifier.pkl\")\n",
    "    \n",
    "    # Initialize video capture from default webcam\n",
    "    video = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Initialize the robust object tracker with a minimum separation distance\n",
    "    tracker = RobustObjectTracker(min_distance=1)  # Helps prevent close object merges\n",
    "    \n",
    "    # Counter for saved image snapshots\n",
    "    counter = 0\n",
    "    \n",
    "    # Ensure output directory exists for saving snapshots\n",
    "    os.makedirs('samples', exist_ok=True)\n",
    "\n",
    "    while True:\n",
    "        # Capture a single frame from the video stream\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break  # Exit loop if frame could not be read\n",
    "\n",
    "        # Detect object bounding boxes from current frame\n",
    "        current_boxes = detect_objects(frame)\n",
    "        \n",
    "        # Update object tracker with newly detected boxes\n",
    "        tracked_objects = tracker.update_tracker(current_boxes)\n",
    "        # tracked_objects = current_boxes  # Can bypass tracking if needed (debug)\n",
    "\n",
    "        # Create a copy of the original frame for visualization\n",
    "        result = frame.copy()\n",
    "        \n",
    "        # Loop over each tracked object\n",
    "        for obj_id, obj_data in tracked_objects.items():\n",
    "            x1, y1, x2, y2 = obj_data['last_box']  # Most recent bounding box\n",
    "            \n",
    "            # Crop the object region from the frame\n",
    "            cropped_patch = frame[y1:y2, x1:x2]\n",
    "            if cropped_patch.size == 0:\n",
    "                continue  # Skip empty patches\n",
    "\n",
    "            # Resize object patch to fixed size required by HOG + classifier\n",
    "            patch = cv2.resize(cropped_patch, (64, 128))\n",
    "            \n",
    "            # If the object label is not yet confident, classify it\n",
    "            if obj_data['confidence'] < 0.7:\n",
    "                features = extract_hog_features(patch)\n",
    "                pred_label = model.predict([features])[0]\n",
    "                confidence = model.predict_proba([features])[0].max()\n",
    "                obj_data['label'] = pred_label\n",
    "                obj_data['confidence'] = confidence\n",
    "            else:\n",
    "                pred_label = obj_data['label']\n",
    "                confidence = obj_data['confidence']\n",
    "            \n",
    "            # Determine bounding box color based on predicted label and confidence\n",
    "            if confidence >= 0.40:\n",
    "                if pred_label == \"book\" and confidence >= 0.20:\n",
    "                    color = (255, 0, 0)          # Black for background\n",
    "                elif pred_label == \"background\":\n",
    "                    color = (0, 0, 0)        # Blue for book\n",
    "                else:\n",
    "                    color = (0, 255, 0)        # Green for other known objects\n",
    "            else:\n",
    "                pred_label = \"unknown\"         # Low confidence fallback\n",
    "                color = (0, 0, 255)            # Red for unknown\n",
    "\n",
    "            # Draw the bounding box and label text\n",
    "            label_text = f\"ID:{obj_id} {pred_label} ({confidence:.2f})\"\n",
    "            cv2.rectangle(result, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(result, label_text, (x1, y1 - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "            \n",
    "            # Draw motion trail using the last 5 recorded bounding boxes\n",
    "            history = obj_data['history'][-5:]\n",
    "            for i in range(1, len(history)):\n",
    "                prev = history[i - 1]\n",
    "                curr = history[i]\n",
    "                cv2.line(result, \n",
    "                         ((prev[0] + prev[2]) // 2, (prev[1] + prev[3]) // 2),\n",
    "                         ((curr[0] + curr[2]) // 2, (curr[1] + curr[3]) // 2),\n",
    "                         color, 2)\n",
    "\n",
    "        # Display the processed result frame in a window\n",
    "        cv2.imshow('Robust Object Tracking', result)\n",
    "        \n",
    "        # Keyboard interaction\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break  # Quit program\n",
    "        elif key == ord('s'):\n",
    "            # Save the current frame as a snapshot\n",
    "            print(\"Saving snapshot...\")\n",
    "            cv2.imwrite(f'final_samples/final_{counter}.jpg', frame)\n",
    "            counter += 1\n",
    "\n",
    "    # Release video stream and close windows after exiting loop\n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d52951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_object_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
